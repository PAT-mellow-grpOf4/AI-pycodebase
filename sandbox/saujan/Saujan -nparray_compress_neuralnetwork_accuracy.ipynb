{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading libraries\n",
    "%matplotlib inline\n",
    "import os\n",
    "from skimage import io\n",
    "import imageio\n",
    "import numpy as np\n",
    "import skimage.transform\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imread, imresize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working directory is /Users/uttam/Dropbox/sandbox-stuffs/college paper-2018_sep/Ai_final works/AI_codebase\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(\"current working directory is %s\" %(cwd))\n",
    "\n",
    "paths    = {\"/Tr20\",\n",
    "            \"/Tr20-valiSets\"\n",
    "           }\n",
    "imgsize  = [78, 68]\n",
    "use_gray = 1\n",
    "\n",
    "compress_name = \"splitDataset\"\n",
    "#converttograyifincasenotchanged\n",
    "def rgb2gray(rgb):\n",
    "    if len(rgb.shape) is 3:\n",
    "        return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "    else:\n",
    "        return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/usr/local/lib/python3.7/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images are 1500 \n"
     ]
    }
   ],
   "source": [
    "nclass     = len(paths)\n",
    "valid_exts = [\".jpg\",\".png\",\".jpeg\"]\n",
    "img_count  = 0\n",
    "for i, relpath in zip(range(nclass), paths):\n",
    "    path = cwd + \"/\" + relpath\n",
    "    flist = os.listdir(path)\n",
    "    for f in flist:\n",
    "        if os.path.splitext(f)[1].lower() not in valid_exts:\n",
    "            continue\n",
    "        fullpath = os.path.join(path, f)\n",
    "        currimg  = imageio.imread(fullpath)\n",
    "        # Converttograyscale  \n",
    "        if use_gray:\n",
    "            grayimg  = rgb2gray(currimg)\n",
    "        else:\n",
    "            grayimg  = currimg\n",
    "        # we will reshapping array\n",
    "        graysmall = skimage.transform.resize(grayimg, [imgsize[0], imgsize[1]])/255\n",
    "        grayvec   = np.reshape(graysmall, (1, -1))\n",
    "        # Save \n",
    "        curr_label = np.eye(nclass, nclass)[i:i+1, :]\n",
    "        if img_count is 0:\n",
    "            totalimg   = grayvec\n",
    "            totallabel = curr_label\n",
    "        else:\n",
    "            totalimg   = np.concatenate((totalimg, grayvec), axis=0)\n",
    "            totallabel = np.concatenate((totallabel, curr_label), axis=0)\n",
    "        img_count    = img_count + 1\n",
    "print (\"Total number of images are %d \" % (img_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split training imges is (900, 5304) \n",
      "split training label is (900, 2) \n",
      "split testing img is (600, 5304) \n",
      "split testing label is (600, 2) \n",
      "Succesfully Saved to /Users/uttam/Dropbox/sandbox-stuffs/college paper-2018_sep/Ai_final works/AI_codebase/compressed_datasets/splitDataset.npz\n",
      "Array compressed\n"
     ]
    }
   ],
   "source": [
    "#we will split diferent datas into train and test for validations\n",
    "def img_gen_shape(string, x):\n",
    "    print (\"split %s is %s \" % (string, x.shape,))\n",
    "    \n",
    "randidx    = np.random.randint(img_count, size=img_count)\n",
    "trainidx   = randidx[0:int(3*img_count/5)]\n",
    "testidx    = randidx[int(3*img_count/5):img_count]\n",
    "\n",
    "training_imdata   = totalimg[trainidx, :]\n",
    "training_labeldata = totallabel[trainidx, :]\n",
    "testing_imdata    = totalimg[testidx, :]\n",
    "testing_labeldata  = totallabel[testidx, :]\n",
    "\n",
    "img_gen_shape(\"training imges\", training_imdata)\n",
    "img_gen_shape(\"training label\", training_labeldata)\n",
    "img_gen_shape(\"testing img\", testing_imdata)\n",
    "img_gen_shape(\"testing label\",testing_labeldata) \n",
    "\n",
    "\n",
    "savepath = cwd + \"/compressed_datasets/\" + compress_name + \".npz\"\n",
    "np.savez(savepath, training_imdata=training_imdata, training_labeldata=training_labeldata \n",
    "         , testing_imdata=testing_imdata, testing_labeldata=testing_labeldata, imgsize=imgsize, use_gray=use_gray)\n",
    "print (\"Succesfully Saved to %s\" % (savepath))\n",
    "print (\"Array compressed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.7.2_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#importing Important libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 train images loaded\n",
      "600 test images loaded\n",
      "5304 dimensional input\n",
      "Image size is [78 68]\n",
      "2 classes\n"
     ]
    }
   ],
   "source": [
    "#loadingcompressedDatasetsfromdirectory\n",
    "cwd = os.getcwd()\n",
    "loadpath = cwd + \"/compressed_datasets/splitDataset.npz\"\n",
    "\n",
    "showP = np.load(loadpath)\n",
    "training_imdata = showP['training_imdata']\n",
    "training_labeldata = showP['training_labeldata']\n",
    "testing_imdata    = showP['testing_imdata']\n",
    "testing_labeldata  = showP['testing_labeldata']\n",
    "ntrain = training_imdata.shape[0]\n",
    "nclass = training_labeldata.shape[1]\n",
    "dim    = training_imdata.shape[1]\n",
    "ntest  = testing_imdata.shape[0]\n",
    "imgsize = showP['imgsize']\n",
    "print (\"%d train images loaded\" % (ntrain))\n",
    "print (\"%d test images loaded\" % (ntest))\n",
    "print (\"%d dimensional input\" % (dim))\n",
    "print (\"Image size is %s\" % (imgsize))\n",
    "print (\"%d classes\" % (nclass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(0)\n",
    "\n",
    "#valuesForrunningSteps\n",
    "learning_rate = 0.200\n",
    "train_Epochs = 100\n",
    "batch_size  = ntrain\n",
    "noOfstep    = 20\n",
    "\n",
    "#hiddenNeuronsDefining\n",
    "n_hidden_1 = 20 #1stlayerinNUmfeatures\n",
    "n_hidden_2 = 20 #2ndlayerinNumfeatures\n",
    "n_input    = dim # data input \n",
    "n_classes  = nclass # total classes (0-9 digits)\n",
    "\n",
    "#tensorValueinputs\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "#mainneuralNetworks\n",
    "def neural_networkModel(_X, _weights, _biases):\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.matmul(_X, _weights['wt1']), _biases['bia1'])) \n",
    "    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, _weights['wt2']), _biases['bia2']))\n",
    "    return tf.matmul(layer_2, _weights['out']) + _biases['out']\n",
    "    \n",
    "#definingOurweightsandBiases\n",
    "stddev = 0.4  \n",
    "weights = {\n",
    "    'wt1': tf.Variable(tf.random_normal([n_input, n_hidden_1], stddev=stddev)),\n",
    "    'wt2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], stddev=stddev)),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes], stddev=stddev))\n",
    "}\n",
    "biases = {\n",
    "    'bia1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'bia2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "pred = neural_networkModel(x, weights, biases)\n",
    "\n",
    "#variableInitializing\n",
    "cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y)) \n",
    "optm = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "corr = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))    \n",
    "accr = tf.reduce_mean(tf.cast(corr, \"float\"))\n",
    "\n",
    "#variableInitializing\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000/100 cost: 8.914620399\n",
      " Training accuracy: 0.288\n",
      " Test accuracy: 0.277\n",
      "Epoch: 020/100 cost: 0.605227590\n",
      " Training accuracy: 0.712\n",
      " Test accuracy: 0.723\n",
      "Epoch: 040/100 cost: 0.618607759\n",
      " Training accuracy: 0.689\n",
      " Test accuracy: 0.727\n",
      "Epoch: 060/100 cost: 0.593586981\n",
      " Training accuracy: 0.720\n",
      " Test accuracy: 0.733\n",
      "Epoch: 080/100 cost: 0.572857916\n",
      " Training accuracy: 0.737\n",
      " Test accuracy: 0.733\n",
      "accuracy test finished\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "#ourtrainingsteps\n",
    "for epoch in range(train_Epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(ntrain/batch_size)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        randidx = np.random.randint(ntrain, size=batch_size)\n",
    "        batch_xs = training_imdata[randidx, :]\n",
    "        batch_ys = training_labeldata[randidx, :]   \n",
    "        # Fit training using batch data\n",
    "        sess.run(optm, feed_dict={x: batch_xs, y: batch_ys})\n",
    "        #calculateAverageloss\n",
    "        avg_cost += sess.run(cost, \n",
    "                feed_dict={x: batch_xs, y: batch_ys})/total_batch\n",
    "        #printingtheresults\n",
    "    if epoch % noOfstep  == 0:\n",
    "        print (\"Epoch: %03d/%03d cost: %.9f\" % \n",
    "               (epoch, train_Epochs, avg_cost))\n",
    "        train_acc = sess.run(accr, feed_dict={x: batch_xs, y: batch_ys})\n",
    "        print (\" Training accuracy: %.3f\" % (train_acc))\n",
    "        test_acc = sess.run(accr, feed_dict={x: testing_imdata, y: testing_labeldata})\n",
    "        print (\" Test accuracy: %.3f\" % (test_acc))\n",
    "\n",
    "print(\"accuracy test finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
